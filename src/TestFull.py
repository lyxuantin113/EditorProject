import os
import cv2
import numpy as np
import torch
from PIL import Image
from facenet_pytorch import MTCNN
from gfpgan import GFPGANer

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'BSRGAN')))
from utils import utils_image as util
from models.network_rrdbnet import RRDBNet as net

# ========== C·∫§U H√åNH ==========
INPUT_IMAGE = '../ImagesOrigin/noise1.jpg'
OUTPUT_IMAGE = f'../ImagesEnhanced/{os.path.basename(INPUT_IMAGE)}_bsrgfp.jpg'
GFPGAN_MODEL_PATH = '../GFPGAN/experiments/pretrained_models/GFPGANv1.4.pth'
BSRGAN_MODEL_PATH = '../BSRGAN/model_zoo/BSRGAN.pth'

# ========== H√ÄM CHU·∫®N H√ìA ·∫¢NH ==========
def ensure_rgb(image_pil):
    try:
        img = image_pil.convert("RGB")
        img_np = np.array(img)
        if len(img_np.shape) == 2:
            img_np = np.expand_dims(img_np, axis=-1)
        if img_np.shape[-1] == 1:
            img_np = np.concatenate([img_np]*3, axis=-1)
        return img_np
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi chu·∫©n h√≥a ·∫£nh: {e}")
        return None

# ========== LOAD BSRGAN ==========
print("üîß ƒêang n√¢ng c·∫•p to√†n ·∫£nh v·ªõi BSRGAN...")

model = net(in_nc=3, out_nc=3, nf=64, nb=23)
model.load_state_dict(torch.load(BSRGAN_MODEL_PATH), strict=True)
model.eval()
model = model.to('cuda' if torch.cuda.is_available() else 'cpu')

device = 'cuda' if torch.cuda.is_available() else 'cpu'

img = Image.open(INPUT_IMAGE)
img_np = ensure_rgb(img)
if img_np is None:
    print("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o.")
    exit()

# Chu·∫©n h√≥a v√† chuy·ªÉn sang tensor
img_tensor = util.uint2tensor4(img_np).to(device)
with torch.no_grad():
    output_tensor = model(img_tensor)
output_np = util.tensor2uint(output_tensor)

print("‚úÖ ·∫¢nh ƒë√£ ƒë∆∞·ª£c n√¢ng c·∫•p b·∫±ng BSRGAN.")

# ========== LOAD GFPGAN ==========
print("ü§ñ ƒêang ph·ª•c h·ªìi khu√¥n m·∫∑t v·ªõi GFPGAN...")

gfpgan = GFPGANer(
    model_path=GFPGAN_MODEL_PATH,
    upscale=1,
    arch='clean',
    channel_multiplier=2,
    bg_upsampler=None
)

mtcnn = MTCNN(keep_all=True, device=device)

# Convert sang PIL v√† detect khu√¥n m·∫∑t
image = Image.fromarray(output_np)
image_np = np.array(image)

boxes, _ = mtcnn.detect(image)
if boxes is None:
    print("‚ùå Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t n√†o.")
    exit()

# ========== X·ª¨ L√ù T·ª™NG KHU√îN M·∫∂T ==========
for i, box in enumerate(boxes):
    pad = 20
    x1, y1 = max(0, int(box[0] - pad)), max(0, int(box[1] - pad))
    x2, y2 = min(image_np.shape[1], int(box[2] + pad)), min(image_np.shape[0], int(box[3] + pad))
    face_crop = image_np[y1:y2, x1:x2].copy()

    _, _, restored_face = gfpgan.enhance(
        face_crop, has_aligned=False, only_center_face=True, paste_back=True
    )

    if restored_face is None:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x·ª≠ l√Ω khu√¥n m·∫∑t t·∫°i box {i}. B·ªè qua...")
        continue

    enhanced_face = Image.fromarray(restored_face).resize((x2 - x1, y2 - y1))
    image.paste(enhanced_face, (x1, y1))

# ========== L∆ØU ·∫¢NH ==========
os.makedirs("../ImagesEnhanced", exist_ok=True)
image.save(OUTPUT_IMAGE)
print(f"‚úÖ ·∫¢nh ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ho√†n ch·ªânh v√† l∆∞u t·∫°i: {OUTPUT_IMAGE}")