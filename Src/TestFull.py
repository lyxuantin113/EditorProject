import os
import cv2
import numpy as np
import torch
from PIL import Image
from facenet_pytorch import MTCNN
from gfpgan import GFPGANer
from basicsr.archs.rrdbnet_arch import RRDBNet
from realesrgan import RealESRGANer

# ========== C·∫§U H√åNH ==========
INPUT_IMAGE = '../ImagesOrigin/dungXem.jpg'
OUTPUT_IMAGE = f'../ImagesEnhanced/{INPUT_IMAGE}_final.jpg'
GFPGAN_MODEL_PATH = '../GFPGAN/experiments/pretrained_models/GFPGANv1.4.pth'
REAL_ESRGAN_MODEL_PATH = '../Real-ESRGAN/weights/RealESRGAN_x2plus.pth'

# ========== H√ÄM CHU·∫®N H√ìA ·∫¢NH ==========
def ensure_rgb(image_pil):
    """
    Chuy·ªÉn ·∫£nh PIL th√†nh np.ndarray RGB chu·∫©n, h·ªó tr·ª£ ·∫£nh grayscale, RGBA, ho·∫∑c ƒë∆°n k√™nh.
    """
    try:
        img = image_pil.convert("RGB")
        img_np = np.array(img)

        if len(img_np.shape) == 2:
            img_np = np.expand_dims(img_np, axis=-1)

        if img_np.shape[-1] == 1:
            img_np = np.concatenate([img_np]*3, axis=-1)

        return img_np
    except Exception as e:
        print(f"‚ö†Ô∏è L·ªói khi chu·∫©n h√≥a ·∫£nh: {e}")
        return None

# ========== LOAD Real-ESRGAN ==========
print("üîß ƒêang n√¢ng c·∫•p background v·ªõi Real-ESRGAN...")

model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)
upscaler = RealESRGANer(
    scale=2,
    model_path=REAL_ESRGAN_MODEL_PATH,
    model=model,
    tile=0,
    tile_pad=10,
    pre_pad=0,
    half=False
)

img = Image.open(INPUT_IMAGE)
img_np = ensure_rgb(img)
if img_np is None:
    print("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o.")
    exit()

bg_enhanced, _ = upscaler.enhance(img_np)
print("‚úÖ Background ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω.")

# ========== LOAD GFPGAN ==========
print("ü§ñ ƒêang ph·ª•c h·ªìi khu√¥n m·∫∑t v·ªõi GFPGAN...")

gfpgan = GFPGANer(
    model_path=GFPGAN_MODEL_PATH,
    upscale=1,
    arch='clean',
    channel_multiplier=2,
    bg_upsampler=None
)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"üì° S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
mtcnn = MTCNN(keep_all=True, device=device)

# Convert sang PIL v√† detect khu√¥n m·∫∑t
image = Image.fromarray(bg_enhanced)
image_np = np.array(image)

boxes, _ = mtcnn.detect(image)
if boxes is None:
    print("‚ùå Kh√¥ng t√¨m th·∫•y khu√¥n m·∫∑t n√†o.")
    exit()

# ========== X·ª¨ L√ù T·ª™NG KHU√îN M·∫∂T ==========
for i, box in enumerate(boxes):
    pad = 20
    x1, y1 = max(0, int(box[0] - pad)), max(0, int(box[1] - pad))
    x2, y2 = min(image_np.shape[1], int(box[2] + pad)), min(image_np.shape[0], int(box[3] + pad))
    face_crop = image_np[y1:y2, x1:x2].copy()

    _, _, restored_face = gfpgan.enhance(
        face_crop, has_aligned=False, only_center_face=True, paste_back=True
    )

    if restored_face is None:
        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ x·ª≠ l√Ω khu√¥n m·∫∑t t·∫°i box {i}. B·ªè qua...")
        continue

    enhanced_face = Image.fromarray(restored_face).resize((x2 - x1, y2 - y1))
    image.paste(enhanced_face, (x1, y1))

# ========== L∆ØU ·∫¢NH ==========
os.makedirs("../ImagesEnhanced", exist_ok=True)
image.save(OUTPUT_IMAGE)
print(f"‚úÖ ·∫¢nh ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ho√†n ch·ªânh v√† l∆∞u t·∫°i: {OUTPUT_IMAGE}")